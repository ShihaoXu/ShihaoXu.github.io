<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="徐世豪" />
    <meta name="generator" content="Pelican (VoidyBootstrap theme)" />

    <title>Computer Vision for Human Computer Interaction - 汪酱的blog</title>

   
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
              integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
              crossorigin="anonymous" />
      <link rel="stylesheet"
            href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
            integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN"
            crossorigin="anonymous">


      <link rel="stylesheet" href="/theme/css/pygment.css" />
      <link rel="stylesheet" href="/theme/css/voidybootstrap.css" />

    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js" integrity="sha384-FFgGfda92tXC8nCNOxrCQ3R8x1TNkMFqDZVQdDaaJiiVbjkPBXIJBx0o7ETjy8Bh" crossorigin="anonymous"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js" integrity="sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
    <![endif]-->

    <link rel="shortcut icon" href="/favicon.ico" />
  </head>

  <body>
   
    <nav class="navbar navbar-default">
      <div class="container">
	   <div class="navbar-header">
		<button type="button" class="navbar-toggle" 
				data-toggle="collapse" data-target="#main-navbar-collapse">
		  <span class="sr-only">Toggle navigation</span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		  <span class="icon-bar"></span>
		</button>
		<a class="navbar-brand" href="/" rel="home">
          <i class="fa fa-home fa-fw fa-lg"> </i> </a>
       </div>

      <div class="collapse navbar-collapse" id="main-navbar-collapse">
        <ul class="nav navbar-nav">
            <li class="">
              <a href="/archives.html">Archives</a>
            </li>
          <li class="divider"></li>
        </ul> <!-- /nav -->
      </div> <!-- /navbar-collapse -->
	  </div> <!-- /container -->
    </nav> <!-- /navbar -->

	<div class="jumbotron" id="overview">
	  <div class="container">
		<h1><a href="/">汪酱的blog</a></h1>
		<p class="lead">I'm just thinking a lot about the site subtitle</p>
	  </div>
	</div>

    <div class="container" id="main-container">
      <div class="row">
        <div class="col-md-9" id="content">
<article itemscope="itemscope" itemtype="http://schema.org/BlogPosting">
  <header class="article-header">
<abbr class="article-header-date">
  Mon 19 February 2018
</abbr> <h1>
  <a href="/drafts/computer-vision-for-human-computer-interaction-de.html" rel="bookmark"
     title="Permalink to Computer Vision for Human Computer Interaction">
    Computer Vision for Human Computer Interaction
  </a>
</h1><div class="article-header-info">
  <p>
      Posted by <a href="/author/shihao-xu.html">Shihao Xu</a>
    in 
    <a href="/category/computer-vision.html">
      Computer Vision</a>
    &nbsp;&nbsp;
  </p>
</div> <!-- /.article-header-info -->  </header>
  <div class="content-body" itemprop="text articleBody">
	<div class="section" id="computer-vision-for-human-computer-interaction-2017-18-ws">
<h2>Computer Vision for Human Computer Interaction 2017/18 WS</h2>
<div class="section" id="vl01-intro-lecture">
<h3>VL01: Intro-Lecture</h3>
<div class="section" id="organisatorisches">
<h4>Organisatorisches</h4>
<ul class="simple">
<li><a class="reference external" href="https://cvhci.anthropomatik.kit.edu/visionhci">Vorlesungswebseite: Computer Vision</a></li>
<li>Ziel der Vorlesung:
Sie sind in der Lage,<ol class="arabic">
<li>Fragestellungen bzgl. der Erfassung von Personen in Bildern und Bildfolgen eigenstaendig zu bearbeiten.</li>
<li>verschiedene grundlegende und aktuelle Verfahren zur Personenerfassung zu erklaeren und zu beurteilen. Und Sie koennen deren Vor- und Nachteile benennen.</li>
</ol>
</li>
<li>Muendlich pruefbar mit 4 SWS / 6 ECTS<ol class="arabic">
<li>Vertiefungsgebiet 11 ''Robotik und Automation''</li>
<li>Vertiefungsgebiet 12 ''Anthropomatik und Kognitive Systeme''</li>
</ol>
</li>
</ul>
<dl class="docutils">
<dt>Programmieraufgaben:</dt>
<dd><p class="first">Drei Programmierprojekte, die in Teams (max. 4 Personen) zu bearbeiten.
Organisation: Manuel Martinez, <a class="reference external" href="mailto:manuel.martinez&#64;kit.edu">mailto:manuel.martinez&#64;kit.edu</a></p>
<ol class="last arabic simple">
<li>Hautfarbe detektieren</li>
<li>Personen detektieren</li>
<li>Gesichtserkennung</li>
</ol>
</dd>
</dl>
<p>Gesamtnote: 90% muendliche Pruefung + 10% Projekte</p>
</div>
<div class="section" id="mensch-computer-interaktion">
<h4>Mensch-Computer Interaktion</h4>
<p>Ein-/Ausgabe</p>
<ol class="arabic simple">
<li>Schalter, Lochkarten</li>
<li>Tastatur, Bildschirm</li>
<li>Maus, Graphische Benutzeroberflaeche</li>
<li>Touchdisplays</li>
<li>Gesten (Zeigegesten, ...)</li>
<li>Sprache</li>
<li>Koerperbewegungen, Mimik</li>
<li>Gaze (Blickrichtung, ...)</li>
</ol>
<p>Wahrnehmende Umgebungen (Smart/Intelligent Environments)</p>
</div>
<div class="section" id="multimodale-interaktion">
<h4>Multimodale Interaktion</h4>
<p>Nutzung von
1. Sprache,
2. Gestik,
3. Blickrichtung
zur Interaktion. (Welches objekt referenziert der Nutzer?)</p>
<p>Mimik (Erkennung von Mimik, &quot;Facial Action Units&quot;)
1. Emotionen / Schmerz / Belastung, Stress
2. Kognitive, psychische Stoerungen</p>
</div>
<div class="section" id="bildverarbeitung-typischer-ablauf">
<h4>Bildverarbeitung - Typischer Ablauf</h4>
<p>Traditional Computer Vision needs hand-crafted feature.</p>
<p>aus Pixel bestehenden Bild (Input data) -&gt; Merkmals extraktion (Feature representation) -&gt; Klassifikation (Learning algorithm) -&gt; Ergebnis</p>
<ul class="simple">
<li>Beispiele fuer ''Merkmale''
- Farbe, Kanten, Segmentierung, Stereobildverarbeitung, Optischer Fluss, lokale Deskriptoren (SIFT), Space-Time-Interest-Points
- Histogramme, Gauss-Misch-Verteilungen, Bag of Words, Fisher Vektoren</li>
<li>Klassifikation -&gt; Maschinelle Lernverfahren
- SVM, ANN, KNN, HMM, Adaboost, Decision Trees, Deep Learning
- ueberwacht/unueberwacht, diskriminativ/generativ</li>
</ul>
</div>
<div class="section" id="deep-learning-learning-feature-hierarchy">
<h4>Deep Learning: Learning feature hierarchy</h4>
<p>Deep learning learns feature hierarchy from end-to-end (from image pixel to classifier output) and train all layers jointly. (Convolutional neural networks)</p>
<p>Input pixel -&gt; low-level feature -&gt; mid-level feature -&gt; high-level feature -&gt; trainable classifier -&gt;</p>
<p>Person/Object Detection with CNN: Input Image -&gt; Extract Region Proposals -&gt; Compute CNN Features -&gt; Classify regions</p>
</div>
<div class="section" id="inhalte-der-vorlesung">
<h4>Inhalte der Vorlesung</h4>
<ul class="simple">
<li>Wiederholung von Grundlagen: Mustererkennung, Bayes-Klassifikation, Gauss-Mischverteilungen, diskriminative Klassifikatoren, SVM</li>
<li>Merkmale und Repraesentationen
- Farbe, Kanten, Segmentierung, Stereobildverarbeitung, Optischer Fluss, lokale Deskriptoren (SIFT), Space-Time-Interest-points
- Hitogramme, Gauss-Mischverteilungen, Bag of Words, Ficher Vektoren</li>
<li>Klassifikatoren/Lernverfahren:
- SVM, NN, AdaBoost, HMM</li>
<li>Verfahren zur Erfassung von Menschen in Bildern und Bildfolgen
- Gesichter Detektieren, Gesichtserkennung
- Gesichtsmodelle und Mimikanalyse
- Personen Detektion
- Kopfdrehung und Aufmerksamkeit
- Gestenerkennung, Erkennung von Handlungen</li>
<li>Trackingansaetze
- Kalman-Filter, Partikel-Filter</li>
</ul>
</div>
</div>
<div class="section" id="vl02-pattern-recognition-basics">
<h3>VL02: Pattern Recognition Basics</h3>
<p>Pattern Recognition</p>
<ul class="simple">
<li>Classifiers:
- Bayes classification
- Gaussian Mixture Models (GMM)
- Linear Discriminant Functions
- SVM
- k-Nearest Neighbours</li>
<li>clustering</li>
</ul>
<dl class="docutils">
<dt>Maschine Learning</dt>
<dd>Learn common patterns based on either a-priori knowledge or on statistical information.</dd>
</dl>
<div class="section" id="bayesian-classification">
<h4>Bayesian Classification</h4>
<p>Problem: Given a feature vector</p>
<div class="math">
\begin{equation*}
P(\omega_i \vert \vec{x}) = \frac{p(\vec{x} \vert \omega_i)P(\omega_i)}{p(\vec{x})}
\text{with } p(\vec{x}=\Sum_{i}p(\vec{x} \vert \omega_i)P(\omega_i))
\end{equation*}
</div>
<div class="math">
\begin{equation*}
posterior = \frac{\text{likelihood} \times \text{prior}}{\text{normalization factor}}
\end{equation*}
</div>
<p>Decide for the class <span class="math">\(\omega_i\)</span> with maximum posterior probability.
Priors describe what we know about the classes before observing anything.</p>
<p>Decision boundary.</p>
</div>
<div class="section" id="gaussian-classification">
<h4>Gaussian Classification</h4>
<p>Assumption: <span class="math">\(p(\vec{x} \vert \omega_i) \sim N(\vec{\mu}, \vec{\Sigma})\)</span>
To reduce parameters, the covariance matrix can be restricted</p>
<p>Estimation of mu and sigma with Maximum Likelihood (ML)</p>
</div>
<div class="section" id="gaussian-mixture-models-gmms">
<h4>Gaussian Mixture Models (GMMs)</h4>
<ol class="arabic simple">
<li>Approximate true density function using a weighted sum of several Guassians.</li>
<li>Any density can be approximated this way with arbitrary precision</li>
</ol>
<blockquote>
<ul class="simple">
<li>Difficult to estimate many parameters
- restrict covariance matrices as before
- Only one shared covariance matrix for all Gaussians</li>
</ul>
</blockquote>
<ol class="arabic simple" start="3">
<li>Estimate parameters of the Gaussians as well as the weights</li>
</ol>
<blockquote>
<ul class="simple">
<li>Expectation Maximization (EM) algorithm</li>
</ul>
</blockquote>
</div>
<div class="section" id="some-taxonomy">
<h4>Some Taxonomy</h4>
<dl class="docutils">
<dt>Parametric vs. non-parametric</dt>
<dd><ol class="first arabic simple">
<li>Parametric: Assume a specific form of probability distribution with some parameters</li>
</ol>
<blockquote>
<ul class="simple">
<li>(+) Need less training data</li>
<li>(-) Only work well if model fits data</li>
</ul>
</blockquote>
<ol class="arabic simple" start="2">
<li>Non-parametric: Parzen windows, k-Nearest neighbors</li>
</ol>
<blockquote class="last">
<ul class="simple">
<li>(+) Work well for all types of distributions</li>
<li>(-) Need more data to correctly estimate distribution</li>
</ul>
</blockquote>
</dd>
</dl>
<p>Generative vs. Discriminative</p>
<blockquote>
<ol class="arabic simple">
<li>Generative Model: A method that models <span class="math">\(P(\omega_i)\)</span> and <span class="math">\(p(\vec{x} \vert \omega_i)\)</span> explicitly</li>
</ol>
<blockquote>
<ul class="simple">
<li>(+) <span class="math">\(p(\vec{x} \vert \omega_i)\)</span> allows to generate new samples of class <span class="math">\(\omega_i\)</span></li>
</ul>
</blockquote>
<ol class="arabic simple" start="2">
<li>Discriminative Model: directly model <span class="math">\(P(\omega_i \vert \vec{x})\)</span> or just output a decision <span class="math">\(\omega_i\)</span> given an input pattern <span class="math">\(\vec{x}\)</span></li>
</ol>
</blockquote>
<p>Linear Discriminant Functions</p>
<blockquote>
<ol class="arabic simple">
<li>Decide two classes <span class="math">\(\omega_1,\ \omega_2\)</span> with a linear hyper plane <span class="math">\(y(x)=w^Tx+w_0\)</span></li>
<li>Examples: Perceptron, Linear SVM</li>
</ol>
</blockquote>
<p>Perceptron algorithm</p>
<blockquote>
<ul class="simple">
<li>Algorithm: Finds a separating hyperplane if data is linearly separable</li>
<li>Perceptron algorithm finds any hyperplane that separates the data</li>
</ul>
</blockquote>
<dl class="docutils">
<dt>SVM</dt>
<dd><p class="first">Find hyperplane that maximizes the margin between the positive and negative examples
Finding the hyperplane with maximum margin can be posed as a constrained optimization problem (quadratic optimization problem)</p>
<blockquote class="last">
<ul class="simple">
<li>Has only one global minimum</li>
<li>Efficient algorithm for solving it is known (optimization using Lagrange multipliers)</li>
</ul>
</blockquote>
</dd>
<dt>SVM Problem: Large margin vs. Training error</dt>
<dd>Noise in data or labels can make training data non-separable
What we want: A way to maximize margin while ignoring outliers
<strong>Hard margin SVM</strong> vs. <strong>Soft Margin SVM</strong>: Trade-Off factor C between low training error and large margin</dd>
<dt>SVM Problem: Really non-separable data -&gt; <strong>non-linear SVM</strong></dt>
<dd><p class="first">Idea: Transform data to high-dimensional space where it is linearly separable
With <strong>Kernel trick</strong>: nothing needs to be computed in high-dimensional space
Common SVM Kernels:</p>
<blockquote>
<ul class="simple">
<li>Linear Kernel</li>
<li>Polynomial Kernel</li>
<li>Gaussian (Radial Basis Function) Kernel</li>
<li>Sigmoid Kernel</li>
</ul>
</blockquote>
<p class="last">Kernels for non-vectorial data are also possible.
Kernel defines a similarity function for the input vectors</p>
</dd>
</dl>
</div>
<div class="section" id="model-selection">
<h4>Model Selection</h4>
<ol class="arabic simple">
<li>SVM Model Selection: Parameter C, Kernel parameters</li>
<li>Training Error is not a good performance measure. We want low test error.</li>
<li><strong>n-fold cross-validation</strong></li>
</ol>
</div>
<div class="section" id="linear-svms">
<h4>Linear SVMs</h4>
<p>If the input space is already high-dimensional, linear SVMs can often perform well too</p>
<dl class="docutils">
<dt>Advantages:</dt>
<dd><ol class="first arabic simple">
<li>Speed: Only one scalar product for classification</li>
</ol>
<blockquote>
<ul class="simple">
<li>Kernel SVM needs #(support vectors) kernel evaluations</li>
</ul>
</blockquote>
<ol class="arabic simple" start="2">
<li>Memory: Only one vector <span class="math">\(\vec{w}\)</span> needs to be stored</li>
</ol>
<blockquote>
<ul class="simple">
<li>Kernel SVM needs to store all support vectors</li>
</ul>
</blockquote>
<ol class="last arabic simple" start="3">
<li>Training: Training is much faster</li>
<li>Model Selection: Only one parameter to optimize</li>
</ol>
</dd>
</dl>
</div>
<div class="section" id="multi-class-svms">
<h4>Multi-Class SVMs</h4>
<p>Generic methods to get a multi-class classifier
* One-vs-all
* one-vs-one</p>
<p>Today real multi-class SVMs are available</p>
</div>
<div class="section" id="k-nearest-neighbors-knn">
<h4>k-nearest-neighbors (KNN)</h4>
<p>Model consists of all training samples.</p>
<dl class="docutils">
<dt>Problems:</dt>
<dd><ol class="first last arabic simple">
<li>Needs enough data</li>
<li>Scalability issues</li>
</ol>
</dd>
</dl>
</div>
<div class="section" id="clustering">
<h4>clustering</h4>
<div class="section" id="k-means">
<h5>K-means</h5>
<dl class="docutils">
<dt>Algorithm</dt>
<dd><ul class="first last simple">
<li>Randomly initialize k cluster centers</li>
<li>Repeat until convergence:
- assign all data points to closest cluster center
- compute new cluster center as mean of of assigned data points</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="agglomerative-hierarchical-clustering-ahc">
<h5>Agglomerative Hierarchical Clustering (AHC)</h5>
<ol class="arabic simple">
<li>Algorithm</li>
</ol>
<blockquote>
<ul class="simple">
<li>Start with one cluster for each data point</li>
<li>Repeat<ul>
<li>merge two closest clusters</li>
</ul>
</li>
</ul>
</blockquote>
<p>Several possibilities to measure cluster distance: min, max, avg, mean</p>
<p>Result is a tree called a dendrogram.</p>
<p>Curse of dimensionality</p>
<blockquote>
<ol class="arabic simple">
<li>Many intuitions about linear algebra are no longer valid in high-dimensional spaces</li>
<li>Classifiers often work better in low-dimensional spaces</li>
</ol>
</blockquote>
<p>In high dimensional space, almost all the space is concentrated in the corners.</p>
</div>
<div class="section" id="dimensionality-reduction">
<h5>Dimensionality reduction</h5>
<dl class="docutils">
<dt>PCA</dt>
<dd>leave out dimensions and minimize error made</dd>
<dt>LDA</dt>
<dd>Maximize class separability</dd>
</dl>
</div>
</div>
</div>
<div class="section" id="vl03-face-detection">
<h3>VL03: Face Detection</h3>
<div class="section" id="color-based-face-detection">
<h4>Color Based Face Detection</h4>
<ul class="simple">
<li>Rationale: human skin has consistent color, which is distinct from many objects</li>
<li>Possible approach:<ol class="arabic">
<li>find skin colored pixels</li>
<li>group skin colored pixels and apply some heuristics to find the face</li>
</ol>
</li>
</ul>
<p>Advantages: Fast, rotation &amp; scale invariant, robust against occlusions
Disadvantages:</p>
<blockquote>
<ul class="simple">
<li>affected by illumination</li>
<li>cannot distinguish head and hands</li>
<li>skin-colored objects in the background problematic</li>
</ul>
</blockquote>
<dl class="docutils">
<dt>Color Spaces</dt>
<dd>RGB, HSI, Class Y Spaces, Perceptually uniform spaces (perceived color difference is uniform to difference in color values),
Chromatic Color Spaces (no intensity information)</dd>
</dl>
<p>Problems</p>
<blockquote>
<ul class="simple">
<li>Reflected color depends on<ul>
<li>spectrum of the light</li>
<li>property of the surface</li>
</ul>
</li>
<li>light source / illumination change</li>
</ul>
</blockquote>
<div class="section" id="how-to-model-skin-color">
<h5>How to model Skin Color</h5>
<ul class="simple">
<li>Non-parametric models (typically histograms)<ul>
<li>Histogram as skin color model<ul>
<li>(+) works very well in practice</li>
<li>(-) memory size quickly gets high</li>
<li>(-) large number of labelled skin and non-skin samples is needed</li>
<li>Classification<ul>
<li>Histogram Backprojection<ul>
<li>Algorithm: compares color of a single pixel with color model</li>
<li>works fine if the color distribution of the target is monomodal</li>
</ul>
</li>
<li>Histogram Matching<ul>
<li>Algorithm: Build a histogram of the image within the search window (image patch) and compare it to the target histogram</li>
<li>Works for multi-modal colored target</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>parametric models<ul>
<li>Gaussian Model<ul>
<li>Gaussian Density Models<ul>
<li>Assumption: The distribution of skin colors <span class="math">\(p(x)\)</span> has a parametric functional form</li>
<li>Mean and covariance matrix are estimated from a training set of skin colors</li>
<li>A color is considered as skin color if <span class="math">\(p(x \vert skin) &gt; \theta\)</span> or <span class="math">\(p(x \vert skin) &gt; p(x \vert non-skin)\)</span></li>
</ul>
</li>
</ul>
</li>
<li>Gaussian Mixture Model<ul>
<li>parameters can be estimated using the Expectation Maximization (EM) algorithm</li>
<li>A color is considered as skin color if <span class="math">\(p(x \vert skin) &gt; \theta\)</span> or <span class="math">\(p(x \vert skin) &gt; p(x \vert non-skin)\)</span></li>
</ul>
</li>
<li>Bayes Classifier<ul>
<li>A color is considered as skin color if <span class="math">\(P(Skin \vert x) &gt; P(non-skin \vert x)\)</span></li>
<li>Decision Rule <span class="math">\(\frac{p(\vec{x} \vert skin)}{p(\vec{x} \vert non-skin)} \ge \frac{P(non-skin)}{P(skin)}\)</span></li>
</ul>
</li>
</ul>
</li>
<li>Discriminative Model, ANN, SVM, ...</li>
</ul>
</div>
</div>
<div class="section" id="performance-measures">
<h4>Performance Measures</h4>
<ol class="arabic simple">
<li>For classification: ROC (Receiver-Operating-Characteristic)</li>
<li>For localization: RPC (Recall-Precision-Curve), DEP (Detection Error Trade-Off)</li>
</ol>
<div class="section" id="roc">
<h5>ROC</h5>
<ol class="arabic simple">
<li>Measures the trade-off between true positive rate and false positive rate</li>
</ol>
<blockquote>
<ul class="simple">
<li>TPR = TP / Pos = TP / (TP + FN)</li>
<li>FPR = FP / Neg = FP / (FP + TN)</li>
</ul>
</blockquote>
<ol class="arabic simple" start="2">
<li>Each prediction hypothesis has generally an associated probability value or score</li>
<li>The performance values can therefore plotted into a graph for each possible score as a threshold</li>
</ol>
</div>
</div>
<div class="section" id="from-skin-colored-pixels-to-faces">
<h4>From Skin-Colored Pixels to Faces</h4>
<p>Skin-Colored pixels need to be grouped into object representations</p>
<div class="section" id="perceptual-grouping">
<h5>Perceptual Grouping</h5>
<p>Morphological Operators</p>
<blockquote>
<ol class="arabic simple">
<li>Operators performs an action on shapes where the input and output is a binary image</li>
<li>Operators: Erosion, Dilatation, Opening, Closing</li>
</ol>
</blockquote>
</div>
</div>
</div>
<div class="section" id="vl04-face-detection-part-ii">
<h3>VL04: Face Detection - Part II</h3>
<p>Idea: use a search-window to scan over an image and train a classifier to decide whether the search window contains a face or not.</p>
<div class="section" id="neual-network-based-face-detection">
<h4>Neual Network Based Face Detection</h4>
<p>Learning: Backpropagation Algorithm</p>
<p>Case Study: Neural Network Based Face Detection 1998</p>
<blockquote>
<ul class="simple">
<li>Idea: use an ANN to detect upright frontal faces</li>
<li>Network:<ul>
<li>scaling the image to detect faces with different sizes into 20x20 (image pyramid)</li>
<li>extracted window: 20x20 pixel region of an image</li>
<li>preprocessing: corrected lighting + histogram equalization</li>
<li>NN input: 20x20 pixels</li>
<li>the NN <tt class="docutils literal">face filter</tt> is applied at very location in the image</li>
<li>4 types of receptive hidden fields</li>
<li>output: real value fomr -1 to +</li>
<li>post-processing: noise removal, merge overlapping detections</li>
</ul>
</li>
<li>Localization and Ground-Truth<ul>
<li>Forlocalization, the test data is mostly annotated with ground-truth bounding boxes</li>
<li>Overlap: <span class="math">\(O = area(GT \cap DET) / area(GT \cup DET)\)</span>, often use 50% as threshold</li>
</ul>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="feature-based-face-detection">
<h4>Feature-Based Face Detection</h4>
<p>Detection based on features, not pixels</p>
<ul class="simple">
<li>Features can encode domain knowledge that is difficult to learn</li>
<li>faster than pixel-based system</li>
<li>scale independent</li>
</ul>
<p>Robust realtime detection of general objects consisting of</p>
<ul class="simple">
<li><strong>Features</strong> for human faces</li>
<li><strong>integral image</strong> to compute features<ul>
<li>speed up: only learn significant features</li>
</ul>
</li>
<li><strong>weak classifier cascade</strong><ul>
<li>AdaBoost is used to boost classification performance of a simple learning algorithm</li>
<li>it combines a collection of weak classifier to form a stronger one</li>
<li>A weak classifier h is a classifier with accuracy only slightly better than chance</li>
<li>Boosting: combing a number of weak classifiers so that the ensemble is arbitrarily accurate</li>
<li><strong>Viola &amp; Jones</strong> approach<ul>
<li>weak classifier <span class="math">\(h_j(x)\)</span> consists of a <strong>feature</strong> <span class="math">\(f_j\)</span>, a <strong>threshold</strong> <span class="math">\(\theta_j\)</span>,
and a <strong>polarity</strong> <span class="math">\(p_j\)</span>: <span class="math">\(h_j(x)=1\)</span>, if <span class="math">\(p_jf_j(x)&lt;p_j\theta_j\)</span>, else <span class="math">\(0\)</span>.</li>
</ul>
</li>
<li>AdaBoost:<ul>
<li>searches over the set of possible weak classifiers and selects those with the lowest classification error</li>
<li>applies a greedy feature selection process</li>
<li>is efficient for selecting a small number of <tt class="docutils literal">good</tt> features with significant variety</li>
</ul>
</li>
</ul>
</li>
<li><strong>train the cascade</strong></li>
</ul>
</div>
</div>
<div class="section" id="vl05-face-recognition-i">
<h3>VL05: Face Recognition I</h3>
<p>Why Choose face recognition over other biometrics?</p>
<ul class="simple">
<li>it requires no physical interaction with the user (non-intrusive method)</li>
<li>it is accurate and allows for high enrollment and verification rates</li>
<li>it doesn't require an expert to interpret the result</li>
<li>it can use your existing hardware (cameras etc)</li>
<li>it is the only biometric that allow to perform passive identification in a one to many scenario</li>
</ul>
<p>From object recognition perspective: It is not general object recognition between different classes, it is a single-class object recognition task (differentiate/verify object within one class).</p>
<div class="section" id="main-problem-challenges">
<h4>Main problem &amp; Challenges</h4>
<ul class="simple">
<li>Extrinsic variations of face images:<ul>
<li>Illumination variations</li>
<li>View-point variations</li>
<li>Occlusions (other objects or people)</li>
</ul>
</li>
<li>Intrinsic variations of face image:<ul>
<li>Facial expressions</li>
<li>Aging</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="face-recognition-tasks">
<h4>Face Recognition Tasks</h4>
<dl class="docutils">
<dt>Closed-Set Identification:</dt>
<dd><p class="first">a biometric task in which an unidentified individual is known to be in the database of biometric characteristics and the system attempts to determine his/her identity.</p>
<p class="last">Performance metric: correct identification rate</p>
</dd>
<dt>Open-Set Identification:</dt>
<dd><ol class="first arabic simple">
<li>decides whether the person on the test image is a known or unknown person</li>
<li>if he is a known person, then who he is?</li>
</ol>
<dl class="last docutils">
<dt>False accept</dt>
<dd>The <strong>invalid identity</strong> is accepted as one of the individuals in the database</dd>
<dt>False reject</dt>
<dd>An individual is rejected <strong>even though</strong> he/she is present in the database</dd>
<dt>False classify</dt>
<dd>An individual in the database is <strong>correctly accepted but misclassified</strong> as one of the other individuals in the training data</dd>
</dl>
</dd>
<dt>Authentication/Verification</dt>
<dd><p class="first">A person claims to be a particular member. The system decides if the person is who he claims to be.</p>
<p>Performance metric:</p>
<ul class="last simple">
<li>false reject rate (FRR): system rejects a valid identity</li>
<li>false accept rate (FAR): system accepts an invalid identity</li>
</ul>
</dd>
</dl>
<p>ROC curve here: FAR against FRR</p>
</div>
<div class="section" id="traditional-approaches">
<h4>Traditional Approaches</h4>
<ul>
<li><p class="first">Feature-based (Geometrical)</p>
<ul class="simple">
<li>fiducial points</li>
<li>distances, angles, areas, etc.</li>
<li>geometrical</li>
</ul>
<p>possible features:</p>
<blockquote>
<ul class="simple">
<li>eyebrow thickness</li>
<li>nose vertical position and width</li>
<li>radii describing the chin shape</li>
</ul>
</blockquote>
<p>Classification</p>
<blockquote>
<p>Nearest neighbor classifier with (todo)Mahalanobis distance as the distance metric:</p>
<blockquote>
<div class="math">
\begin{equation*}
\Delta_j(x)=(x-m_j)^T\Sigma^{-1}(x-m_j)
\end{equation*}
</div>
<p><span class="math">\(x\)</span>: input face image
<span class="math">\(m_j\)</span>: average vector representing the j-th person
<span class="math">\(\Sigma\)</span>: Covariance matrix</p>
</blockquote>
<p>Different people are characterized only by their average feature vector.</p>
<p>The Distribution is common and estimated by using all the examples in the training set.</p>
</blockquote>
</li>
<li><p class="first">appearance-based</p>
<ul class="simple">
<li>holistic (i.e. process the whole fase as the input) , local/fiducial regions (i.e. precess facial features, such as eyes, mouth, etc. separately)</li>
<li>statistical</li>
</ul>
<p>Preprocessing step</p>
<blockquote>
<p>align faces with facial landmarks</p>
<blockquote>
<ul class="simple">
<li>e.g. using manually labeled or automatically detected eye centers</li>
<li>normalize face images to a common coordination, remove translation, rotation and scaling factors</li>
<li>crop off unnecessary background</li>
</ul>
</blockquote>
</blockquote>
</li>
</ul>
</div>
<div class="section" id="eigenfaces">
<h4>Eigenfaces</h4>
<dl class="docutils">
<dt>Eigenface</dt>
<dd><p class="first">A face image defines a point in the high dimensional image space</p>
<p>They can be described by a relatively low dimensional subspace</p>
<p>Project the face images into an appropriately chosen subspace and perform classification by similarity computation</p>
<p>Dimensionality reduction procedure used here is called Karhunen-Loeve transformation or Principal Component analysis (PCA)</p>
<p class="last"><strong>Objective</strong>: Find the vectors that best account for the distribution of face images within the entire image space.</p>
</dd>
</dl>
<div class="section" id="pca">
<h5>PCA</h5>
<dl class="docutils">
<dt>Pricipal components (are called eigenfaces in the context of Eigenface, they span the ''Face Space'')</dt>
<dd>the eigenvectors of the covariance matrix of the set of face images.</dd>
</dl>
<p>Goal of PCA:</p>
<blockquote>
<ul class="simple">
<li>Find direction vectors (using covariance matrix) so as to minimize the average projection error</li>
<li>Project on the largest K direction vectors, which spans a linear subspace, to reduce dimensionality</li>
</ul>
</blockquote>
<p>Eigenface</p>
<blockquote>
<p>Training:</p>
<blockquote>
<ol class="arabic">
<li><p class="first">Acquire initial set of face images (training set),:</p>
<p><span class="math">\(Y\)</span>: face images</p>
<p><span class="math">\(y_i\)</span>: one face image</p>
<div class="math">
\begin{equation*}
Y = [y_1, y_2, ...,y_K]
\end{equation*}
</div>
</li>
<li><p class="first">Calculate the eigenfaces from the training set, keeping only the M images corresponding the highest eigenvalues</p>
<div class="math">
\begin{equation*}
m = \frac{1}{K} \times \sum_{i=1}^Ky
\end{equation*}
</div>
<div class="math">
\begin{equation*}
C = (Y - m)(Y - m)^T
\end{equation*}
</div>
<div class="math">
\begin{equation*}
D = U^T C U
\end{equation*}
</div>
<p><span class="math">\(m\)</span>: mean face</p>
<p><span class="math">\(C\)</span>: covariance matrix</p>
<p><span class="math">\(D\)</span>: eigenvalues</p>
<p><span class="math">\(U\)</span>: eigenvectors</p>
</li>
<li><p class="first">Calculate representation of each known individual <span class="math">\(k\)</span> in face space</p>
<div class="math">
\begin{equation*}
\Omega_k = U^T \times (y_k - m)
\end{equation*}
</div>
</li>
</ol>
</blockquote>
<p>Testing:</p>
<blockquote>
<p>Project input new image <span class="math">\(y\)</span> into face space: <span class="math">\(\Omega = U^T \times (y - m)\)</span></p>
<p>class = <span class="math">\(\operatorname*{arg\,min}_k||\Omega - \Omega_k||\)</span></p>
</blockquote>
</blockquote>
</div>
<div class="section" id="projections-onto-the-face-space">
<h5>Projections onto the face space</h5>
<ul>
<li><p class="first">Images can be reconstructed by their projections in face space: <span class="math">\(Y_f = \sum_{i=1}^M \omega_i u_i\)</span></p>
</li>
<li><p class="first">Appearance of faces in face-space does not change a lot</p>
</li>
<li><p class="first">Difference of mean-adjusted image <span class="math">\((Y-m)\)</span> and projection <span class="math">\(Y_f\)</span> gives a measure of ''faceness''</p>
<dl class="docutils">
<dt>dffs</dt>
<dd><p class="first last">distance from face space, can be used to detect faces</p>
</dd>
</dl>
</li>
<li><p class="first">Cases:</p>
<ul>
<li><p class="first">case 1: projection of a known individual</p>
<blockquote>
<p>near face space and near known face</p>
</blockquote>
</li>
<li><p class="first">case 2: projection of an unknown individual</p>
<blockquote>
<p>near face space, far from reference vectors</p>
</blockquote>
</li>
<li><p class="first">case 3: not a face</p>
<blockquote>
<p>far from face space</p>
</blockquote>
</li>
</ul>
</li>
<li><p class="first">Project all face onto a universal eigenspace (using PCA) to ''encode'' via pricipal components. Then use inverse-distance as a similarity measure <span class="math">\(S(p,g)\)</span> for matching &amp; recognition</p>
</li>
</ul>
<dl class="docutils">
<dt>View-based Eigenspaces</dt>
<dd><p class="first">Extension: view-based eigenspaces (parametric eigenspace) for general viewing conditions.</p>
<ol class="last arabic simple">
<li>Build an eigenspace for each view</li>
<li>decide input images' direction of view using distance from view space metric</li>
<li>do classification in that view-space</li>
</ol>
</dd>
</dl>
</div>
</div>
<div class="section" id="bayesian-face-recognition">
<h4>Bayesian Face Recognition</h4>
<p>Problem: Simple nearest-neighbour similarity measures do not exploit knowledge of critical appearance variations</p>
<dl class="docutils">
<dt>Bayesian similarity measure:</dt>
<dd>Denotes belief that image differences are caused by typical appearance variations of an individual (caused by expression, etc). Compares typical within-class (intrapersonal) variations with between-class (extrapersonal) variations</dd>
</dl>
<div class="section" id="dual-pca-bayesian">
<h5>Dual PCA (Bayesian)</h5>
</div>
</div>
</div>
<div class="section" id="vl08-introduction-to-deep-learning">
<h3>VL08: Introduction to Deep Learning</h3>
<div class="section" id="traditional-computer-vision">
<h4>Traditional computer vision</h4>
<p>Input data (pixels) -&gt; Feature representation (hand-crafted) -&gt; Learning algorithm (e.g. SVM)</p>
<p>Features are not learned.</p>
<p>Popular computer vision features</p>
<blockquote>
<ul class="simple">
<li>SIFT</li>
<li>HoG</li>
<li>Gabor filters</li>
<li>and many others: SURF, LBP, Color Histograms, GLOH, ...</li>
</ul>
</blockquote>
<p>Mid-level representations</p>
<blockquote>
<ul class="simple">
<li>mid-level cues: continuation, parallelism, junctions, corners</li>
<li>object parts</li>
</ul>
</blockquote>
<p>are difficult to create, how about learning them?</p>
</div>
<div class="section" id="learning-feature-hierarchy">
<h4>Learning feature hierarchy</h4>
<p>Learn a hierarchy (hierarchical representation of data) from end-to-end (from image pixels to classifier output), and train all layers jointly:</p>
<blockquote>
<ul class="simple">
<li>low-level feature</li>
<li>mid-level feature</li>
<li>high-level feature</li>
<li>trainable classifier</li>
</ul>
</blockquote>
</div>
<hr class="docutils" />
<div class="section" id="neural-networks">
<h4>Neural Networks</h4>
<div class="section" id="simple-neuron-model">
<h5>Simple Neuron Model</h5>
<p><span class="math">\(out = activation(\vec{x}^T \cdot \vec{w})\)</span></p>
</div>
<div class="section" id="nn-topology">
<h5>NN Topology</h5>
<ul class="simple">
<li>Fully connected</li>
<li>Feed-forward</li>
<li>Recurrent</li>
</ul>
</div>
<div class="section" id="forward-propagation">
<h5>Forward propagation</h5>
</div>
</div>
<div class="section" id="convolutional-neural-networks">
<h4>Convolutional Neural Networks</h4>
<p>Handwritten digits recognition by LeCun et. al. 1989: multiple convolution and fully connected layers.</p>
<ul>
<li><dl class="first docutils">
<dt>Fully Connected Layer</dt>
<dd><p class="first">Too many model parameters! (200x200 pixels for 40k neurons = 1.6 Billion parameters)</p>
<blockquote class="last">
<ul class="simple">
<li>Waste of resources</li>
<li>no local correlation</li>
<li>insufficient data</li>
</ul>
</blockquote>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Convolutional Layer</dt>
<dd><p class="first">Reduced number of parameters (200x200 pixels, 96 filter kernels, 11x11 filter size = 11616 parameters)</p>
<blockquote class="last">
<ul class="simple">
<li>connect a hidden unit to image patch</li>
<li>share weights</li>
<li>use multiple filters</li>
<li>Receptive field: The receptive field of an individual sensory neuron is the particular region of the sensory space (e.g., the body surface, or the visual field) in which a stimulus will modify the firing of that neuron.</li>
</ul>
</blockquote>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="vl14-person-detection-i-global-approaches">
<h3>VL14: Person Detection I - Global Approaches</h3>
<dl class="docutils">
<dt>People detection</dt>
<dd>People detection can be used to implement person tracking. It works when faces not visible, and still works when resolution is too low for faces. Difficulties are: clothing, (occlusions by) accessories, articulation (different poses), clutter: overlap each other (crowds). People detection is tightly coupled with general object detection techniques.</dd>
</dl>
<div class="section" id="category-i-still-image-vs-video">
<h4>Category I - still image vs. video</h4>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Still image based person detection</th>
<th class="head">video based person detection</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><ul class="first last simple">
<li>Mostly based on gray-value information from image</li>
<li>Other possible cues: color, infra-red, radar, stereo</li>
<li>(-) Often more difficult (only a single frame)</li>
<li>(-) Performs poorer than video based techniques</li>
<li>(+) Applicable in wider variety of applications</li>
</ul>
</td>
<td><ul class="first last simple">
<li>Background modeling</li>
<li>Temporal information (speed, position in earlier frame</li>
<li>Optical flow</li>
<li>Can be (re-)initialized by still image approach</li>
<li>(-) hard to apply in unconstrained scenarios
(e.g. moving cameras or changing bckgrounds</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="category-ii-global-vs-parts">
<h4>Category II - global vs. parts</h4>
<table border="1" class="docutils">
<colgroup>
<col width="49%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">global approaches</th>
<th class="head">part-based approaches</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><ul class="first last simple">
<li>holistic model, e.g. one feature for whole person</li>
<li>(+) typically simple model</li>
<li>(+) work well for low resolutions</li>
<li>(-) problems with occlusions</li>
<li>(-) problems with articulations</li>
</ul>
</td>
<td><ul class="first last simple">
<li>Background modeling model body sub-parts separately</li>
<li>(+) deal better with moving body parts (poses)</li>
<li>(+) able to handel occlusions, overlaps</li>
<li>(+) sharing of training data</li>
<li>(-) require more complex reasoning</li>
<li>(-) problem with lowresolutions</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="catetory-iii-discriminative-vs-generative">
<h4>Catetory III - discriminative vs. generative</h4>
<table border="1" class="docutils">
<colgroup>
<col width="49%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">generative model</th>
<th class="head">part-based approaches</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><ul class="first last simple">
<li>models how data (i.e. person images) is generated</li>
<li>(+) possibly interpretable,
i.e. know why reject/accept</li>
<li>(+) models the object class/can draw samples</li>
<li>(-) model variability unimportant to classification</li>
<li>(-) often hard to build good model with
few parameters</li>
</ul>
</td>
<td><ul class="first last simple">
<li>can only discriminate for given data, if it is a
person or not</li>
<li>(+) appealing when infeasible to model data itself</li>
<li>(+) currently often excel in practice</li>
<li>(-) often can't provide uncertainty in predictions</li>
<li>(-) non-interpretable</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="typical-components-of-global-approaches">
<h4>Typical components of global approaches</h4>
<p>Detection via classification, i.e. a binary classifier</p>
<blockquote>
<ul class="simple">
<li>Gradient based, e.g. HOG + SVM</li>
<li>Edge based, e.g. todo: Chamfer Silhouette Matching + Decision Stump (threshold)</li>
<li>Wavelet based, e.g. Wavelet Features + SVM</li>
</ul>
</blockquote>
<dl class="docutils">
<dt>How to turn a classifier into a detector</dt>
<dd>sliding window: scan window at different <strong>positions and scales</strong></dd>
<dt>Gradient Histogram (GradHist)</dt>
<dd><p class="first">extremly popular and successful in the vision community. It avoid hard decisions (compared to edge based features)</p>
<p>Some examples:</p>
<ul class="last simple">
<li>HOG: Histogram of Oriented Gradients</li>
<li>SIFT: Scale-Invariant Feature Transform</li>
<li>GLOH: Gradient Location and Orientation Histogram</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="gradhist-step-1-computing-gradients">
<h4>GradHist step 1: computing gradients</h4>
<ul class="simple">
<li>one sided &amp; two sided <span class="math">\(f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}\)</span>, <span class="math">\(f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x-h)}{2h}\)</span></li>
<li>Image: discrete, 2-dimensional signal</li>
<li>filter masks in x-direction, one and two sided: <tt class="docutils literal"><span class="pre">[-1,</span> 1]</tt> and <tt class="docutils literal"><span class="pre">[-1,</span> 0, 1]</tt></li>
<li>gradient magnitude: <span class="math">\(s=\sqrt{s_x^2 + s_y^2}\)</span></li>
<li>gradient orientation: <span class="math">\(\theta=\operatorname*{arctan}\frac{s_y}{s_x}\)</span></li>
</ul>
</div>
<div class="section" id="gradhist-step-2-gradient-histograms">
<h4>GradHist step 2: gradient histograms</h4>
<ul class="simple">
<li>Gradient histograms measure the orientation and strength of gradients within an image region</li>
<li>bins can be filled by absolute number of pixels or weighted according to gradient magnitudes</li>
</ul>
</div>
<div class="section" id="the-hog-people-detector-dalal-cvpr05">
<h4>The HOG People Detector <a class="citation-reference" href="#dalal-cvpr05" id="id1">[Dalal-cvpr05]</a></h4>
<ul>
<li><p class="first">Gradient-based feature descriptor developed for people detection</p>
</li>
<li><p class="first">Global descriptor for the complete body</p>
</li>
<li><p class="first">high-dimensional (typically ~4000 dimensions)</p>
</li>
<li><p class="first">very promising results on challenging data sets</p>
</li>
<li><p class="first">phases:</p>
<ol class="arabic simple">
<li>Learning Phase</li>
</ol>
<blockquote>
<p>normalised train image data set (set of cropped images containing pedestrians in normal environments) -&gt;
encode images into feature spaces (global descriptor -- ~4000 dimensional vector -- for each input image/person) -&gt;
learn binary classifier (train a linear SVM) -&gt;
object/non-object decision</p>
</blockquote>
<ol class="arabic simple" start="2">
<li>Detection Phase</li>
</ol>
<blockquote>
<p>scan image at all scales and locations (sliding window over each scale) -&gt;
run classifier to obtain object/non-object decisions (simple SVM prediction) -&gt;
fuse multiple detections in 3-D position &amp; scale space (clustering) -&gt;
object detections with bounding boxes</p>
</blockquote>
</li>
</ul>
<div class="section" id="overview-hog-descriptor">
<h5>Overview HOG Descriptor</h5>
<ol class="arabic simple">
<li>compute gradients on an image region of 64x128 pixels</li>
<li>compute gradient orientation histograms on <strong>cells</strong> of 8x8 pixels (in total 8x16 cells). typical histogram size: 9 bins.</li>
<li>normalize histograms within overlapping <strong>blocks</strong> of 2x2 cells (in total 7x15 blocks) block descriptor size: 4x9 = 36</li>
</ol>
<blockquote>
<ul class="simple">
<li>Cell histograms are oncatenated and then normalized</li>
</ul>
</blockquote>
<ol class="arabic simple" start="4">
<li>concatenate <strong>block descriptors</strong> 7 x 15 x 4 x 9 = 3780 dimensional feature vector</li>
</ol>
<ul class="simple">
<li>Feature Engineering<ul>
<li>Developing a feature descriptor requires a lot of engineering: testing of parameters, normalization schemes</li>
<li>An extensive evaluation of different choices was performed, when the descriptor was proposed</li>
</ul>
</li>
<li>From feature to detector<ul>
<li>Simple linear SVM on top of the HOG Features: fast (one inner product per evaluation window)</li>
<li>Gaussian kernel SVM: slightly better classification accuracy but considerable increases computation time</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="classifying-a-hypothesis-roc">
<h5>Classifying a hypothesis: ROC</h5>
<p>One kind of ROC curve: TPR against FPR</p>
<dl class="docutils">
<dt>TPR (true positive rate)</dt>
<dd>TP/Pos = TP/(TP+FN)</dd>
<dt>FPR (false positive rate)</dt>
<dd>FP/Neg = FP/(FP+TN)</dd>
<dt>Precision (percentage of correct classified positive examples w.r.t. all examples, which are claimed to be positives by the classifier)</dt>
<dd>TP/(TP+FP)</dd>
<dt>Recall (= TPR, percentage of correct classified positive examples w.r.t. all positive examples)</dt>
<dd>TP/(TP+FN)</dd>
<dt>1 - Precision (= False Discovery Rate)</dt>
<dd>FP/(FP+TP)</dd>
</dl>
</div>
</div>
<div class="section" id="silhouette-matching-gavrila-iccv99">
<h4>Silhouette Matching <a class="citation-reference" href="#gavrila-iccv99" id="id2">[Gavrila-ICCV99]</a></h4>
<div class="section" id="main-idea">
<h5>Main Idea</h5>
<ul class="simple">
<li>Goal: Align known object shapes with image</li>
<li>Requirements for an alignment algorithm<ul>
<li>high detection rate</li>
<li>few false positives</li>
<li>robustness</li>
<li>computationally inexpensive</li>
</ul>
</li>
<li>input edge image &lt;--&gt; silhouette database</li>
</ul>
<p>todo?: Computational Complexity = O(#positions * #templates * #contourpixels * sizeof(searchregion))</p>
</div>
</div>
<div class="section" id="distance-transform-dt">
<h4>Distance Transform (DT)</h4>
<ul class="simple">
<li>used to compared/align two (typically binary) shapes<ol class="arabic">
<li>compute the distance from each pixel to the nearest edge pixel in the image of shape 1(</li>
<li>Overlay second shape over distance transform</li>
<li>accumulate distances along shape 2</li>
<li>find best matching position by an exhaustive search</li>
</ol>
</li>
</ul>
<div class="section" id="chamfer-matching">
<h5>Chamfer Matching</h5>
<ul class="simple">
<li>Compute distance transform (DT)</li>
<li>For each possible object location<ul>
<li>position known object shape over DT</li>
<li>accumulate distances along the contour</li>
<li>keep instances where the accumulated distance is below some threshold</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="distance-measure">
<h5>Distance measure</h5>
<div class="math">
\begin{equation*}
dist = \frac{1}{N}\sum_{i\in F}dt(i),
\end{equation*}
</div>
<div class="math">
\begin{equation*}
F:features, N=|f|
\end{equation*}
</div>
</div>
<div class="section" id="efficient-implementation">
<h5>Efficient Implementation</h5>
<p>Similar detailed efficient implementation: <a class="reference external" href="https://goo.gl/uMGW7i">Mathmatical Morphology and Distance Transform</a></p>
<p>The distance transform can be efficiently computed by two scans over the complete image</p>
<ul>
<li><p class="first">Forward-scan</p>
<p>The local distances in the mask are added to the pixel values of the distance map and the new value of the zero pixel
is the minimum of the five sums.</p>
</li>
<li><p class="first">Backward-scan</p>
</li>
</ul>
</div>
<div class="section" id="advantages-and-disadvantages">
<h5>Advantages and disadvantages</h5>
<ul class="simple">
<li>Fast<ul>
<li>Distance transform has to be computed only once</li>
<li>comparison for each shape location is cheap</li>
</ul>
</li>
<li>Good performance on uncluttered images (with few background structures)</li>
<li>addition advantage of silhouette matching: not just bounding box, but segmentation!</li>
<li>bad performance for cluttered images</li>
<li>needs a huge number of people silhouettes in order to cover all poses, computation effort increases with the number of silhouettes</li>
</ul>
</div>
<div class="section" id="template-hierarchy-similar-to-cascade">
<h5>Template Hierarchy (Similar to cascade)</h5>
<ul class="simple">
<li>Goal: Recude the number of silhouettes to consider</li>
<li>Approach: Organize silhouettes in a template hierarchy, the shapes are clustered by similarity (by the shape clustering process)</li>
<li>template hierarchy is similar to (todo?) cascades</li>
</ul>
</div>
<div class="section" id="coarse-to-fine-search-similar-to-cascade">
<h5>Coarse-To-Fine Search (Similar to cascade)</h5>
<p>Goal: Reduce search effort by discarding unlikely regions with minimal computational effort</p>
<p>Idea</p>
<ul class="simple">
<li>subsample the image and search first at a coarse scale</li>
<li>only consider regions with a low distance when searching for a match on finer scales</li>
<li>reasonable threshold needed</li>
<li>also similar to cascades</li>
</ul>
</div>
<div class="section" id="adding-edge-orientation">
<h5>Adding edge orientation</h5>
<p>Idea: Consider edge orientation for each pixel</p>
<ul>
<li><p class="first">Chamfer distance given two shapes S, C:</p>
<div class="math">
\begin{equation*}
d_{chamfer}(S,C) = \frac{1}{n}\sum_{s_i \in S}\min_{c_j \in C} ||s_i - c_j||
\end{equation*}
</div>
</li>
<li><p class="first">The orientation similarity between two points:</p>
<div class="math">
\begin{equation*}
p(s_i, c_j) = K * [\tan (\alpha_{s_i} - \beta_{c_j})]^2
\end{equation*}
</div>
</li>
<li><p class="first">The combined distance measure:</p>
<div class="math">
\begin{equation*}
d_{chamfer}(S, C) = \frac{1}{n}\sum_{s_i \in S} \rho (\frac{1}{k}||s_i - c(s_i)|| + p(s_i, c(s_i)))
\end{equation*}
</div>
</li>
</ul>
<p>where <span class="math">\(c(s_i)\)</span> is the closest contour point to point <span class="math">\(s_i\)</span></p>
</div>
</div>
</div>
<div class="section" id="vl15-people-detection-ii-part-based-approaches">
<h3>VL15: People Detection II - Part-Based Approaches</h3>
<dl class="docutils">
<dt>HOG</dt>
<dd>Global model, 4000 dim. feature vector, SVM classifier, sliding window, image pyramid</dd>
<dt>Chamfer Matching</dt>
<dd>silhouettes (global model), distance transform, hierarchy, coarse-to-fine</dd>
</dl>
<div class="section" id="part-based-models">
<h4>Part-Based Models</h4>
<p>Motivation motivation part-based models:</p>
<ul class="simple">
<li>break down an object's overall variability into more manageable pieces (by using less complex classifiers, apply
prior knowledge by (manually) splitting the global object into meaningful parts</li>
<li>(+) deal better with moving body parts</li>
<li>(+) able to handel occlusions, overlaps</li>
<li>(+) sharing of training data</li>
<li>(-) require more complex reasoning</li>
<li>(-) problems with low resolutions</li>
</ul>
<div class="section" id="approche-first-proposed-in-1873-by-fischler">
<h5>Approche First Proposed in 1873 by Fischler</h5>
<p>Model has two main components</p>
<ul class="simple">
<li>parts (2D image fragments)</li>
<li>structure (configuration of parts)<ul>
<li>fixed spatial layout</li>
<li>flexible spatial layout<ul>
<li>can better handle deformations or articulation changes</li>
<li>well suited for non-rigid objects</li>
<li>spatial relations are often modeled probabilistically</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="the-mohan-people-detector">
<h5>The Mohan People Detector</h5>
<ul class="simple">
<li>4 parts<ul>
<li>face and shoulder</li>
<li>legs</li>
<li>right arm</li>
<li>left arm</li>
</ul>
</li>
<li>fixed layout<ul>
<li>best location has to be found for each detection window</li>
</ul>
</li>
<li>combination: classifier (SVM)</li>
<li>detection<ul>
<li>sliding window approach</li>
<li>64x128 pixels</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="the-implicit-shape-model-ism">
<h5>The Implicit Shape Model (ISM)</h5>
<p>Main Ideas:</p>
<ol class="arabic simple">
<li>automatically learn a large number of <strong>local parts</strong> that occur on the object (also referred to as <strong>visual vocabulary</strong>, <strong>bag of words</strong>, or <strong>codebook</strong>)</li>
<li>learn a star-topology structural model</li>
</ol>
<blockquote>
<ul class="simple">
<li>features are considered independent given the objects' center</li>
<li>likely relative positions are learned from data</li>
</ul>
</blockquote>
<p>Steps:</p>
<ol class="arabic simple">
<li>Part Detection/Localization</li>
</ol>
<blockquote>
<ul>
<li><p class="first">part definition - manual vs. learned</p>
<p>Questions:</p>
<ul class="simple">
<li>Do such parts model the variability of body in an optimal way</li>
<li>any possible non-semantic parts can model human parts better</li>
</ul>
<p>Can we develop an Approach that automatically finds the parts it needs</p>
</li>
<li><p class="first">Necessity for part decomposition: repeatable, distinctive, compact, efficient, cover</p>
</li>
</ul>
</blockquote>
<ol class="arabic simple" start="2">
<li>Part Description</li>
</ol>
<blockquote>
<ul>
<li><p class="first">local descriptors: describe local region around a keypoint</p>
</li>
<li><p class="first">orientation invariance</p>
<ul class="simple">
<li>compute orientation histogram</li>
<li>select dominant orientation</li>
<li>normalize: rotate to fixed orientation</li>
</ul>
</li>
<li><p class="first">the SIFT Descriptor</p>
<p>histogram of gradient orientations</p>
<ul class="simple">
<li>captures important texture information</li>
<li>robust to small translations / affine deformations</li>
<li>region rescaled to a grid of 16x16 pixels (8x8 in image)</li>
<li>4x4 regions (2x2 in image) = 16 histograms (concatenated)</li>
<li>histograms: 8 orientation bins, gradients weighted by gradient magnitude</li>
<li>final descriptor has 128 dimensions and is normalized to compensate for illumination differences</li>
</ul>
</li>
<li><p class="first">Shape Context Descriptor</p>
<ul class="simple">
<li>count the number of points inside each bin</li>
<li>log-polar binning: more precision for nearby points, more flexibility for farther points</li>
</ul>
</li>
</ul>
<p>Harris-/Hessian-Laplace/DoG work well for many natural categories.</p>
</blockquote>
<ol class="arabic simple" start="3">
<li>Learning Part Appearances</li>
</ol>
<blockquote>
<p>Visual Vocabulary</p>
<ul>
<li><p class="first">Compute</p>
<ol class="arabic simple">
<li>Detect keypoints on all person training examples</li>
<li>compute local descriptors for all keypoints</li>
</ol>
<p>Result: Large set of local image descriptors that all occur on People</p>
</li>
<li><p class="first">Group visually similar local descriptors</p>
<ul class="simple">
<li>Grouping Algorithms / Clustering<ul>
<li>Partitional Clustering<ul>
<li>K-means</li>
<li>Gaussian Mixture Clustering (EM)</li>
</ul>
</li>
<li>Hierarchical of Agglomerative Clustering</li>
</ul>
</li>
<li>parts, that occur only rarely are discarded</li>
<li>visual vocabulary size<ul>
<li>cover as much of the body and many poses as possible</li>
<li>vocabulary size 10000+ clusters (probabilistic votes decide, whether part is important or not)</li>
</ul>
</li>
</ul>
<p>Result: descriptor groups representing human body parts</p>
</li>
<li><p class="first">Note: The term &quot;Visual Vocabulary&quot; was coined, because the method is inspired by word vocabularies used in speech processing</p>
<ul class="simple">
<li>Bag of Words: image represented as a &quot;bag&quot; of the visual words that appear in it</li>
</ul>
</li>
</ul>
</blockquote>
<ol class="arabic simple" start="4">
<li>Learning the Spatial Layout of Parts</li>
</ol>
<blockquote>
<p>Spatial Occurrence (Star-Model)</p>
<blockquote>
<ul class="simple">
<li>Record spatial occurrence<ul>
<li>match vocabulary entires to training images</li>
<li>record occurrence distributions with respect to object center</li>
</ul>
</li>
<li>Generalized Hough Transform<ul>
<li>For every feature, store possible &quot;occurrences&quot;</li>
<li>For new image, let the matched features vote tfor possible object positions</li>
</ul>
</li>
</ul>
</blockquote>
</blockquote>
<ol class="arabic simple" start="5">
<li>Combination of Part Detections</li>
</ol>
<dl class="docutils">
<dt>ISM Detection Procedure</dt>
<dd>Image -&gt; Probabilistic Voting -&gt; 3D Voting Space -&gt; Back-Projection -&gt; Segmentation -&gt; Detection Confidences</dd>
<dt>Probabilistic Formulation</dt>
<dd>Marginalization over all found descriptors gives the probability or detection confidence of an object at any location.</dd>
</dl>
</div>
</div>
<div class="section" id="local-features">
<h4>Local Features</h4>
<div class="section" id="local-features-keypoints-and-descriptors">
<h5>local features - keypoints and descriptors</h5>
<p>Two components of local features:</p>
<ul class="simple">
<li>key- or interest-points -&gt; Where?<ul>
<li>specify repeatable points on the object</li>
<li>consists of x/y-position and scale</li>
</ul>
</li>
<li>local (keypoint) descriptors  -&gt; How does it look like?<ul>
<li>describe the area around an interest point</li>
<li>define the feature representation of an interest point</li>
</ul>
</li>
<li>property of local features: repeatable, distinctive, compact, efficient</li>
</ul>
<p>Local Feature - General Approach</p>
<ul class="simple">
<li>find keypoints using keypoint detector</li>
<li>define region around keypoint</li>
<li>normalize region</li>
<li>compute local descriptor</li>
<li>compare descriptors</li>
</ul>
</div>
<div class="section" id="local-features-keypoint-detection">
<h5>local features - keypoint detection</h5>
<p>Keypoint Detectors:</p>
<blockquote>
<p>Many Existing Detectors:</p>
<blockquote>
<ul class="simple">
<li>Hessian &amp; Harris<ul>
<li>looking for two-dimensional signal changes (strong second derivatives in two orthogonal directions)</li>
<li>responses mainly on corners and strongly textured area.</li>
</ul>
</li>
<li>Laplacian, DoG</li>
<li>Harris-/Hessian-Laplace</li>
<li>Harris-/Hessian-Affine</li>
<li>EBR and IBR</li>
<li>MSER</li>
<li>Salient Regions</li>
</ul>
</blockquote>
</blockquote>
<ul class="simple">
<li>Scale Space<ul>
<li>With e.g. Hessian Detector can we detect repeatable points in the image.</li>
<li>Can we not only detect a distinctive position, but also a characteristic scale around an interest point?</li>
</ul>
</li>
<li>Scale Invariance<ul>
<li>Automatic Scale Selection (Scale Signature)</li>
<li>a useful signature function: Laplacian-of-Gaussian = &quot;blob&quot; detector</li>
<li>local maxima in <strong>scale space</strong> of Laplacian-of-Gaussian</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="vl16-tracking">
<h3>VL16: Tracking</h3>
<p>Motivation:</p>
<ul class="simple">
<li>Use more than one image to analyse the scene</li>
<li>Use a-priori knowledge to improve analysis (system dynamics, imaging / measurement process)</li>
</ul>
<p><a class="reference external" href="https://www.youtube.com/watch?v=RWl1KZY65q8">Detection vs Tracking</a></p>
<dl class="docutils">
<dt>Detection</dt>
<dd>Detect the object <strong>independently</strong> in each frame</dd>
<dt>Tracking</dt>
<dd><strong>predict</strong> the new location of the object in the next frame using <strong>estimated dynamics</strong> Then we <strong>update</strong> based upon measurements.</dd>
</dl>
<table border="1" class="docutils">
<colgroup>
<col width="54%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Tracking</th>
<th class="head">Detection</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><ul class="first last simple">
<li>determine a target's <strong>state</strong> (location, rotation, deformation,
pose, ...) <strong>over a sequence of observations</strong> derived from images</li>
<li>priveds object positions in each frame</li>
</ul>
</td>
<td><ul class="first last simple">
<li>find an object in a <strong>single image</strong></li>
<li>no assumption about dynamics, temporal consistency made</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Target types</p>
<ul class="simple">
<li>Tracking single objects</li>
<li>Multiple objects</li>
<li>Articulated body</li>
</ul>
<p>Sensor Setup</p>
<ul class="simple">
<li>Single Camera</li>
<li>Multiple Cameras<ul>
<li>wide baseline</li>
<li>narrow bseline (stereo)</li>
</ul>
</li>
<li>Active Cameras<ul>
<li>pan, tilt, zoom</li>
<li>moving cameras (robot)</li>
</ul>
</li>
<li>Cameras + Microphones</li>
</ul>
<p>Some observations used for tracking: templates, color, foreground-background segmentation, edges, dense disparity, optical flow, detectors (body, body parts)...</p>
<p>Tracking as State Estimation:</p>
<ul class="simple">
<li>Predict state of the system (position, pose, ...), whose state cannot directly be measured. Only certain observations (measurements) can be made (with noise/error).</li>
<li>What is the most likely state $x$ of the system at a given time, given a sequence of observations <span class="math">\(Z_t\)</span>?<ul>
<li><span class="math">\(\operatorname*{arg\,max}\limits_{x_t} p(x_t | Z_t)\)</span></li>
</ul>
</li>
</ul>
<div class="section" id="bayes-filter">
<h4>Bayes Filter:</h4>
<ul>
<li><p class="first">Assume state x to be Markov process</p>
<div class="math">
\begin{equation*}
p(x_t | x_{t-1},x_{t-2},...,x_0)=p(x_t | x_{t-1})
\end{equation*}
</div>
</li>
<li><p class="first">State <span class="math">\(x\)</span> generate observations <span class="math">\(z\)</span></p>
<div class="math">
\begin{equation*}
p(z_t | x_t, x_{t-1},...,x_0)=p(z_t | x_t)
\end{equation*}
</div>
</li>
<li><p class="first">Want to estimate most likely state <span class="math">\(x_t\)</span> given sequence <span class="math">\(Z_t\)</span>: <span class="math">\(\operatorname*{arg\,max} p(x_t|Z_t)\)</span></p>
</li>
<li><p class="first">Can be estimated recursively</p>
</li>
<li><p class="first">Predict step:</p>
<div class="math">
\begin{equation*}
p(x_t | Z_{t-1})= \int_{x_{t-1}}p(x_t|x_{t-1})p(x_{t-1}|Z_{t-1})dx_{t-1}
\end{equation*}
</div>
</li>
<li><p class="first">Update step:</p>
<div class="math">
\begin{equation*}
p(x_t|Z_t)=\alpha p(z_t|x_t)p(x_t|Z_{t-1})
\end{equation*}
</div>
</li>
<li><p class="first">Needed</p>
<ul class="simple">
<li>Process model: <span class="math">\(p(x_t|x_{t-1})\)</span></li>
<li>Measurement model: <span class="math">\(p(z_t | x_t)\)</span></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="karlman-filter">
<h4>Karlman Filter</h4>
<ul class="simple">
<li>The Kalman Filter is an instance of a Bayes Filter</li>
<li>Assumptions:<ul>
<li>Linear state propagation and measurement model</li>
<li>Gaussian process and measurement noise</li>
</ul>
</li>
</ul>
<p>The simple Kalman Filter is not applicable, when the process to be estimated is not linear or the measurement relationship to the process is not linear.</p>
<div class="section" id="tracking-one-face-with-a-kalman-filter">
<h5>Tracking one face with a Kalman filter</h5>
<p>State: (x, y, scale)
Observations: skin color</p>
</div>
</div>
<div class="section" id="particle-filter">
<h4>Particle Filter</h4>
<ul class="simple">
<li>The Kalman Filter often fails when the measurement density is multimodal/non-Gaussian.</li>
<li>A particle filter represents and propagates arbitrary probability distributions. They are represented by a set of weighted samples.</li>
<li>The particle filter is a <strong>numerical</strong> technique, whereas the Kalman filter is analytical.</li>
<li>Like a kalman filter, a particle filter incorporates a <strong>dynamic model</strong> describing system dynamics</li>
</ul>
<p>Probability Density Function is represented by weighted samples (&quot;particles&quot;)</p>
<p>For a PF tracker, you need</p>
<ul class="simple">
<li>a set of <span class="math">\(\vec{N}\)</span> weighted samples (particle) at time <span class="math">\(k\)</span>: <span class="math">\(\{(s_k^{(i)},\pi_k^{(i)})\vert i=1..N\}\)</span></li>
<li>the motion model: <span class="math">\(s_k^{(i)} \leftarrow s_{k-1]^{(i)}\)</span></li>
<li>the observation model: <span class="math">\(\pi_k^{(i)} \leftarrow s_k^{(i)}\)</span></li>
</ul>
<div class="section" id="the-condensation-algorithm">
<h5>The Condensation Algorithm</h5>
<p>A popular instance of a PF in computer vision.</p>
<ol class="arabic simple">
<li>Select. Randomly select new samples from the old sample set accroding to their weights.</li>
<li>Predict: Propagate the samples using the motion model.</li>
<li>Measure: Calculate weights for the new samples using the observation model.</li>
</ol>
<p>The PF tracks a probability distribution - how to get the target position?</p>
<ul class="simple">
<li>(Calculate the weighted mean of the particle set)</li>
<li>Cluster the particle set and search for the highest mode</li>
<li>just take the strongest particle</li>
</ul>
<p>How many particles are needed?</p>
<ul class="simple">
<li>depends strongly on the dimension of the state space!</li>
<li>Tracking 1 object in the image plane typically requires 50-500 particles.</li>
</ul>
</div>
</div>
<div class="section" id="examples">
<h4>Examples</h4>
<ul class="simple">
<li>Tracking one face with a particle filter</li>
<li>Tracking Multiple objects, two approaches<ul>
<li>a dedicated tracker for each of the objects</li>
<li>a single tracker in a joint state space</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="summary">
<h4>Summary:</h4>
<ul class="simple">
<li>What is tracking?</li>
<li>Tracking a state estimation <span class="math">\(p(x \vert Z)\)</span></li>
<li>Kalman Filter: transition and measurement matrix, process and measurement noise</li>
<li>Particle Filter: motion model, observation model, condensation algorithm, particle vs. Kalman filter</li>
<li>Examples<ul>
<li>single object tracking: face</li>
<li>multiple objects: faces and head poses<ul>
<li>one filter per object</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="vl17-tracking-ii">
<h3>VL17: Tracking II</h3>
<p>Multi-Camera Topologies</p>
<blockquote>
<ul class="simple">
<li>Stereo-Camera system (narrow baseline)<ul>
<li>allows for calculation of a dense disparity map</li>
</ul>
</li>
<li>Wide-Baseline multi-camera system<ul>
<li>allows for 3D localization of objects in the joint field of view</li>
</ul>
</li>
<li>Multi-camera network</li>
</ul>
</blockquote>
<div class="section" id="d-to-2d-projection">
<h4>3D to 2D projection</h4>
<div class="section" id="perspective-projection">
<h5>Perspective Projection</h5>
<ul class="simple">
<li>4 parameters have to be known to perform the projection.</li>
<li>They are called &quot;internal camera parameters&quot;</li>
<li>Perform calibration to estimate those parameters.</li>
</ul>
</div>
<div class="section" id="calibration-intrinsics">
<h5>Calibration - Intrinsics</h5>
<p>The intrinsic parameters describe the optical properties of each camera.</p>
<ul class="simple">
<li>f: the focal length</li>
<li><span class="math">\(c_x, c_y\)</span>: the principal point (&quot;optical center&quot;)</li>
<li><span class="math">\(K_1, ..., K_n\)</span>: distortion parameters</li>
</ul>
</div>
<div class="section" id="calibration-extrinsics">
<h5>Calibration - Extrinsics</h5>
<p>The extrinsic parameters describe the location of each camera with respect to a global coordinate system:</p>
<ul class="simple">
<li>T: the translation vector</li>
<li>R: the 3x3 rotation matrix</li>
</ul>
</div>
<div class="section" id="camera-calibration">
<h5>Camera Calibration</h5>
<ol class="arabic simple">
<li>For each camera: A calibration target with a known geometry is captured from multiple views</li>
<li>The corner points are extracted (semi-)automatically</li>
<li>The locations of the corner points are used to estimate the intrinsics iteratively</li>
<li>Once the intrinsics are known, a fixed calibration target is captured from all of the cameras (extrinsics)</li>
</ol>
<dl class="docutils">
<dt>Triangulation</dt>
<dd>Assumption: the object location is known in multiple views</dd>
</dl>
<p>Background Models</p>
<ul class="simple">
<li>Prerequisite: the scene is static</li>
<li>Capture the scene without the object to be tracked -&gt; background model. Each pixel that differs significantly from its counterpart in the background model belongs to the target.</li>
<li>Problem (-&gt; Background Model Adaptation):<ul>
<li>objects are being moved</li>
<li>the camera is being moved</li>
<li>illumination changes</li>
</ul>
</li>
</ul>
<dl class="docutils">
<dt>Simple Background Model Adaptation</dt>
<dd>Background = mean of the past n frames + alpha process (exponentially average)</dd>
</dl>
<p>Foreground Segmentation / Triangulation</p>
<blockquote>
<ul class="simple">
<li>Object location represented by center or top-most point of the silhouette</li>
<li>Correspondence Problem<ul>
<li>Solution: Multi-camera tracking without explicit triangulation</li>
</ul>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="audio-visual-tracking-of-a-speaker">
<h5>Audio-Visual Tracking of a Speaker</h5>
<ul class="simple">
<li>Target: the speaker in a lecture</li>
<li>Sensors: multiple fixed cameras and microphones</li>
<li>Features: background subtraction, face and upper body detection, GCC of the audio signal</li>
<li>Tracking Scheme: Particle Filter</li>
<li>Tracking Scheme, Condensation ALgorithm with ...<ul>
<li>State Space: the speaker's head centroid</li>
<li>Motion model: particles are propagated using Gaussian diffusion</li>
<li>Observation model: a particle's weight is determined by local visual and acoustical information gathered from all cameras and from all microphone pairs</li>
<li>Visual Features</li>
<li>Acoustic features<ul>
<li>TDOA: time-delay-of-arrival</li>
<li>GCC: generalized cross correlation function, calculate the signal correlation given the TDOA</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="vl18-kinect-body-pose">
<h3>VL18: Kinect Body-Pose</h3>
<p>Kinect:</p>
<ul class="simple">
<li>Fusion of two groundbreaking new technologies:<ul>
<li>A cheap and fast RGB-D sensor</li>
<li>A reliable Skeleton Tracking</li>
</ul>
</li>
<li>Features<ul>
<li>Type: 3D</li>
<li>Distance: 4m</li>
<li>Frequency: 30Hz</li>
<li>Resolution: 320 x 240</li>
<li>Price: 150 Euro</li>
</ul>
</li>
</ul>
<p>RGB-D: Time-of-Flight (TOF) Cameras</p>
<blockquote>
<ul class="simple">
<li>Digital ToF cameras contain an array of high speed digital counters</li>
<li>Example ZCam:<ul>
<li>320 x 240 ~ 30GHz counters</li>
<li>60 FPS Depth</li>
<li>Claimed precision: 1-2cm</li>
<li>Claimed range: 2.5m</li>
<li>Was meant to cost less than $70</li>
</ul>
</li>
<li><a class="reference external" href="https://goo.gl/jVHHbh">ToF Cameras</a></li>
<li>Primesence (体感操控 公司名，研发了Kinect，被微软收购）</li>
<li>RGB-D hardward design:<ul>
<li>Depth: IR + Depth Image CMOS</li>
<li>RGB: RGB Sensor</li>
<li>Projector</li>
</ul>
</li>
<li>RGB-D: Kinect specs:<ul>
<li>1st device: Primesense Board<ul>
<li>1280 x 1024 CMOS color bayer sensor 30fps&#64;vga</li>
<li>1280 x 1024 CMOS IR sensor 30fps&#64;vga</li>
<li>PS1080 which delivers Depth at 30fps&#64;vga</li>
</ul>
</li>
<li>2nd &amp; 3rd devices: microphones, noise cancellation, tile &amp; accelerometer.</li>
<li>Power and security devices</li>
</ul>
</li>
</ul>
</blockquote>
<p>Kinect: Structured light</p>
<ul class="simple">
<li>Kinect uses Structured Light to simulate a stereo camera system</li>
</ul>
<p>RGB-D: Block Matching</p>
<ul class="simple">
<li>IR pattern is projected by an temperature stabilized IR laser</li>
<li>IR sensor has a mtching, very narrow bandpass filter</li>
</ul>
<table class="docutils citation" frame="void" id="dalal-cvpr05" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[Dalal-cvpr05]</a></td><td>Dalal, N.; Triggs, B.; Histograms of oriented gradients for human detection; CVPR 2005</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="gavrila-iccv99" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[Gavrila-ICCV99]</a></td><td>Gavrila, D.M.; Philomin, V.; Real-time object detection for &quot;smart&quot; vehicles; ICCV 1999</td></tr>
</tbody>
</table>
</div>
</div>
<script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  
<div class="article-tag-list">
<span class="label label-default">Tags</span>
	<a href="/tag/computer-vision.html"><i class="fa fa-tag"></i>Computer Vision</a>&nbsp;
	<a href="/tag/machine-learning.html"><i class="fa fa-tag"></i>Machine Learning</a>&nbsp;
</div>  <hr />
  <div class="well well-sm">  <!-- Social media sharing buttons -->

    <!-- Google+ -->
    <div class="g-plus" data-action="share" data-annotation="bubble"></div>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <div class="g-plusone" data-size="medium"></div>&nbsp;

    <!-- Facebook -->
    <div class="fb-like" 
        data-href="/drafts/computer-vision-for-human-computer-interaction-de.html" 
        data-layout="button_count" 
        data-action="like" data-show-faces="true" 
        data-share="true">
    </div>
    &nbsp;
  </div> <!-- /Social media sharing buttons -->
</article>
        </div><!-- /content -->

        <div class="col-md-3 sidebar-nav" id="sidebar">

<div class="row">

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-comment fa-fw fa-lg"></i> Social</h4>
<ul class="list-unstyled social-links">
    <li><a href="https://www.flickr.com/people/150485183@N02/" target="_blank">
	  <i class="fa fa-flickr fa-fw fa-lg" title="Flickr"></i>
		Flickr
	</a></li>
    <li><a href="pages/images/wechat-QRcode.JPG" target="_blank">
	  <i class="fa fa-weixin fa-fw fa-lg" title="WeChat"></i>
		WeChat
	</a></li>
</ul>
</div>

<div class="col-xs-6 col-md-12">
<h4><i class="fa fa-folder fa-fw fa-lg"></i> Categories</h4>
<ul class="list-unstyled category-links">
  <li><a href="/category/reading-notes.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Reading Notes</a></li>
  <li><a href="/category/vorlesung.html" >
    <i class="fa fa-folder-open fa-fw fa-lg"></i> Vorlesung</a></li>
</ul>
</div>

</div> <!-- /row -->

  <h4><i class="fa fa-link fa-fw fa-lg"></i> Links</h4>
  <ul class="list-unstyled category-links">
    <li><a href="http://getpelican.com/" >
      <i class="fa fa-fw fa-external-link-square fa-lg"></i> Pelican</a></li>
  </ul>
<h4><i class="fa fa-tags fa-fw fa-lg"></i> Tags</h4>
<p class="tag-cloud">
    <span class="tag-4">
      <a href="/tag/keras.html">
          <i class="fa fa-tag"></i>
        Keras
      </a>
    </span>
    <span class="tag-1">
      <a href="/tag/deep-learning.html">
          <i class="fa fa-tag"></i>
        Deep Learning
      </a>
    </span>
    <span class="tag-4">
      <a href="/tag/scikit-learn.html">
          <i class="fa fa-tag"></i>
        Scikit-Learn
      </a>
    </span>
    <span class="tag-4">
      <a href="/tag/machine-learning.html">
          <i class="fa fa-tag"></i>
        Machine Learning
      </a>
    </span>
    <span class="tag-4">
      <a href="/tag/tensorflow.html">
          <i class="fa fa-tag"></i>
        TensorFlow
      </a>
    </span>
    <span class="tag-1">
      <a href="/tag/shell-script.html">
          <i class="fa fa-tag"></i>
        Shell Script
      </a>
    </span>
    <span class="tag-4">
      <a href="/tag/mikroprozessor.html">
          <i class="fa fa-tag"></i>
        Mikroprozessor
      </a>
    </span>
</p>

<hr />

        </div><!--/sidebar -->
      </div><!--/row-->
    </div><!--/.container /#main-container -->

    <footer id="site-footer">
 
      <address id="site-colophon">
        <p class="text-center text-muted">
        Site built using <a href="http://getpelican.com/" target="_blank">Pelican</a>
        &nbsp;&bull;&nbsp; Theme based on
        <a href="http://www.voidynullness.net/page/voidy-bootstrap-pelican-theme/"
           target="_blank">VoidyBootstrap</a> by 
        <a href="http://www.robertiwancz.com/"
           target="_blank">RKI</a>  
        </p>
      </address><!-- /colophon  -->
    </footer>


    <!-- javascript -->
   
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
            integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
            crossorigin="anonymous"></script>


<!-- Facebook -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script>

<!-- Twitter -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<!-- Google+ -->
<!-- Synchronous 
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>
-->
<!-- Asynchronous -->
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/platform.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>  </body>
</html>